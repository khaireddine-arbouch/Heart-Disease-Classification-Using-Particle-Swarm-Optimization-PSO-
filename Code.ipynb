{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "415a1716-91ee-4373-9584-22d756a10e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas numpy scikit-learn matplotlib keras tensorflow pyswarms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyswarms.single.global_best import GlobalBestPSO\n",
    "import pyswarms as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f493f-d906-410f-a10c-c240237165fb",
   "metadata": {},
   "source": [
    "# Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0e5ea3a-b6ee-47ae-99e0-30f21e4765c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_data():\n",
    "    # Assuming the dataset is in the same directory and named 'heart_disease_data.csv'\n",
    "    data = pd.read_csv('heart_statlog_cleveland_hungary_final.csv')\n",
    "    X = data.drop('target', axis=1)\n",
    "    y = data['target']\n",
    "    return X, y\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18587576-5cd3-41f4-ac20-4a0a15075e7c",
   "metadata": {},
   "source": [
    "# Implement neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6188cce6-588a-46c1-912d-d1342543b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network structure\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        self.w1 = np.random.rand(n_input, n_hidden)\n",
    "        self.w2 = np.random.rand(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = np.dot(X, self.w1)\n",
    "        a1 = np.tanh(z1)  # Activation function for hidden layer\n",
    "        z2 = np.dot(a1, self.w2)\n",
    "        a2 = np.exp(z2) / np.sum(np.exp(z2), axis=1, keepdims=True)  # Softmax for output\n",
    "        return a2\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        split = self.w1.size\n",
    "        self.w1 = weights[:split].reshape(self.w1.shape)\n",
    "        self.w2 = weights[split:].reshape(self.w2.shape)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Make predictions\n",
    "        proba = self.forward(X)\n",
    "        return np.argmax(proba, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4894f57-ff40-4eb3-8b4e-9c1c32394a7d",
   "metadata": {},
   "source": [
    "# PSO Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b85117c1-2895-4285-b105-807d62e69f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 20:01:48,833 - pyswarms.single.global_best - INFO - Optimize for 1000 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best: 100%|██████████████████████████████████████████████████████████████████████████████████████████████|1000/1000, best_cost=0.103\n",
      "2024-05-19 20:03:18,361 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.1028248315625541, best pos: [ 1.18949260e+00  2.50310657e-01  2.82950595e-02 -5.99632190e-02\n",
      "  7.67239183e-01  9.99291968e-02  1.60131139e+00  2.28177834e-01\n",
      "  2.47571894e+00 -2.62768397e-01  3.47619878e-01  3.67176811e-02\n",
      "  5.65469143e-01  2.91216287e-01  1.45274234e+00  2.54903502e-01\n",
      "  5.12465682e-01  1.00783112e+00  1.33062263e-01  8.16110131e-02\n",
      "  1.07371125e+00 -4.39037425e-01  1.43521289e+00 -2.58760101e-01\n",
      " -4.80316673e-01  4.64958744e-01  1.68355826e+00  9.89789730e-01\n",
      "  6.49832090e-01  2.14955604e-01  9.21869720e-01  5.70447556e-01\n",
      " -5.78941965e-01  2.65988522e-01  8.13091033e-01  9.65753345e-01\n",
      "  5.16558846e-01  7.86756110e-01  6.95872285e-01  5.99316020e-01\n",
      "  2.78804234e-01  5.75545671e-01 -8.64645352e-02 -2.59040574e-01\n",
      "  3.82092840e-01  1.21985974e-01  4.73497185e-01 -5.05545060e-01\n",
      "  9.41188080e-01  1.59488351e+00  4.77048881e-01  1.43839598e+00\n",
      "  1.01787302e-03  3.77426245e-01  8.08847205e-01  5.85850940e-01\n",
      "  9.60547451e-01  9.66994890e-01  8.86176280e-03  1.72941981e+00\n",
      "  8.56751120e-01 -4.00108573e-01 -1.03218403e-01 -1.97046001e-01\n",
      "  9.20950187e-01  6.31164732e-01  8.33517791e-01  1.30407756e+00\n",
      "  9.88696341e-01  5.13811449e-01 -4.61220843e-02 -2.58800769e-01\n",
      "  3.67687578e-01  5.71058913e-02  8.99699869e-01  7.58417190e-01\n",
      "  1.40143766e+00 -1.66668623e-01  5.66546225e-01  7.71840228e-01\n",
      "  6.99344440e-01  6.71898716e-01  6.04070242e-01  1.23735861e-01\n",
      " -4.70547113e-01  1.57148000e+00  1.06211435e+00 -1.68864341e-02\n",
      " -3.27813554e-01  4.10286372e-01 -1.03965353e+00  4.34425219e-01\n",
      "  6.43496447e-01  3.09369870e-01  1.35619191e-01  4.61438316e-01\n",
      "  1.12960905e+00  6.34895060e-01  8.88565353e-01  7.05951128e-01\n",
      " -7.29383337e-01  2.56738349e-01  6.69881085e-01  2.56346371e+00\n",
      " -3.55647340e-02  1.45269708e+00  7.67414876e-02 -4.90665069e-01\n",
      "  6.47860521e-01  6.86896990e-01 -5.80246495e-02  3.92181257e-01\n",
      " -2.36611523e-01  9.73724067e-01  9.13015236e-02  1.86855078e+00\n",
      " -2.57044925e-01  7.90367132e-01  7.72961298e-01  8.79773853e-01\n",
      "  2.29902451e-01  1.97587905e-01  9.59206871e-01  1.01163978e-01\n",
      " -1.26489407e-01  7.07245333e-01  6.07441608e-01  7.46579005e-01\n",
      "  8.75859627e-01  1.32631425e-01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8529411764705882\n"
     ]
    }
   ],
   "source": [
    "# Objective function to minimize\n",
    "def f_per_particle(weights, nn):\n",
    "    nn.set_weights(weights)\n",
    "    y_pred = nn.forward(X_train)\n",
    "    loss = np.mean((y_train_encoded - y_pred) ** 2)  # Mean squared error\n",
    "    return loss\n",
    "\n",
    "# Constraint for PSO\n",
    "def f(x, nn):\n",
    "    n_particles = x.shape[0]\n",
    "    return np.array([f_per_particle(x[i], nn) for i in range(n_particles)])\n",
    "\n",
    "# Main execution function\n",
    "def train_network(X, y):\n",
    "    global X_train, y_train_encoded  # Make these available globally for PSO evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    y_train_encoded = np.eye(np.max(y_train) + 1)[y_train]  # One-hot encoding of labels\n",
    "    \n",
    "    nn = NeuralNetwork(X_train.shape[1], 10, 2)  # 10 hidden nodes, 2 output nodes\n",
    "    optimizer = GlobalBestPSO(n_particles=30, dimensions=(nn.w1.size + nn.w2.size), options={'c1': 0.5, 'c2': 0.3, 'w': 0.9})\n",
    "    best_cost, best_pos = optimizer.optimize(lambda x: f(x, nn=nn), iters=1000, verbose=3)\n",
    "    \n",
    "    nn.set_weights(best_pos)\n",
    "    y_pred = nn.predict(X_test)\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Load data, preprocess and train the network (assuming load_data and preprocess functions are already defined)\n",
    "X, y = load_data()\n",
    "X = preprocess(X)\n",
    "train_network(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bfcdc2-1e18-4fb7-81dc-0478e641139f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The implementation of various optimization techniques on the heart disease dataset yielded significant results. Among the techniques tested, Particle Swarm Optimization (PSO) achieved the highest accuracy of 85.29%, demonstrating its effectiveness in optimizing neural network weights. This was followed by other methods such as Genetic Algorithm, Simulated Annealing, and Randomized Hill Climbing, each contributing valuable insights into the optimization process. The comparative analysis highlights the strengths and potential of using advanced optimization techniques for improving the performance of neural network models in medical data classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
